# ============================================================================
# Storyteller AI — Environment Configuration
# ============================================================================
# Copy this file to .env and adjust values as needed.
# All variables have sensible defaults; only change what you need.
#
# Documentation: docs/architecture.md
# ============================================================================

# ── Core Paths ──────────────────────────────────────────────────────────────
# STORYTELLER_DATA_ROOT=./data           # Root data directory
# STORYTELLER_DB_PATH=./data/storyteller.db  # SQLite database path
# VECTORDB_PATH=                         # LanceDB path (auto-resolves ./data/lancedb)
# ERA_PACK_DIR=./data/static/era_packs   # Era pack directory

# ── Feature Flags ───────────────────────────────────────────────────────────
# ENABLE_BIBLE_CASTING=1                 # Era Pack deterministic NPC casting
# ENABLE_PROCEDURAL_NPCS=1               # Deterministic procedural NPC fallback
# ENABLE_SUGGESTION_REFINER=1            # LLM-based KOTOR-style suggestions
# ENABLE_CLOUD_BLUEPRINT=0               # Cloud LLM for campaign blueprint (requires cloud provider)
# ENABLE_SCALE_ADVISOR=0                 # Dynamic campaign scale advisor
# ENABLE_CHARACTER_FACETS=0              # Character facets system (experimental)

# ── LLM Provider ────────────────────────────────────────────────────────────
# Default: all roles use Ollama (local). No cloud API keys needed.
# Timeout for LLM requests (seconds):
# LLM_TIMEOUT=300
# OLLAMA_TIMEOUT=300

# ── Per-Role Model Configuration ────────────────────────────────────────────
# Pattern: STORYTELLER_{ROLE}_MODEL=<model_name>
# Override the LLM model for a specific agent role.
#
# STORYTELLER_ARCHITECT_MODEL=qwen3:4b
# STORYTELLER_DIRECTOR_MODEL=mistral-nemo:latest
# STORYTELLER_NARRATOR_MODEL=mistral-nemo:latest
# STORYTELLER_CASTING_MODEL=qwen3:4b
# STORYTELLER_BIOGRAPHER_MODEL=qwen3:4b
# STORYTELLER_MECHANIC_MODEL=qwen3:8b
# STORYTELLER_SUGGESTION_REFINER_MODEL=qwen3:8b
# STORYTELLER_KG_EXTRACTOR_MODEL=qwen3:4b
# STORYTELLER_EMBEDDING_MODEL=nomic-embed-text
# STORYTELLER_CAMPAIGN_INIT_MODEL=qwen3:4b

# To use a custom Ollama base URL (e.g., remote server):
# STORYTELLER_DIRECTOR_BASE_URL=http://my-gpu-server:11434

# ── Embedding & RAG ────────────────────────────────────────────────────────
# EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
# EMBEDDING_DIMENSION=384
# LORE_TABLE_NAME=lore_chunks
# STYLE_TABLE_NAME=style_chunks
# CHARACTER_VOICE_TABLE_NAME=character_voice_chunks
# BASE_STYLE_SOURCE=star_wars_base_style

# ── Universe Modularity ────────────────────────────────────────────────────
# Override setting-specific bypass methods (comma-separated):
# SETTING_BYPASS_METHODS=magic,potion,enchantment
# DEFAULT_SETTING_ID=star_wars_legends
# ERA_PACK_LENIENT_VALIDATION=1

# ── Development & Debugging ────────────────────────────────────────────────
# DEV_CONTEXT_STATS=0                    # Include RAG context stats in API response
# INGESTION_TAGGER_ENABLED=0             # Enable LLM-based ingestion tagging
# NPC_RENDER_ENABLED=0                   # Enable NPC portrait rendering
# MECHANIC_SEED=                         # Fixed seed for mechanic determinism (testing)
# ENCOUNTER_SEED=                        # Fixed seed for encounter determinism (testing)
# STORYTELLER_DUMMY_EMBEDDINGS=          # Use dummy embeddings (testing only)
